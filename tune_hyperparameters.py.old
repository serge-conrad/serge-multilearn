
from sklearn.ensemble import RandomForestClassifier

from sklearn.naive_bayes import GaussianNB
from skmultilearn.problem_transform import BinaryRelevance
from sklearn.neural_network import MLPClassifier



import json
import os
from invoke import task
from sklearn.pipeline import make_pipeline
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.multioutput import MultiOutputClassifier
from config import PARAMS_FILE,param_grids  # Import the global variable


def save_hyperparameters(best_classifiers, dataset):
    """Save the best hyperparameters for a specific dataset to a JSON file."""
    # Prepare a serializable dictionary
    serializable_classifiers = {}

    for classifier_name, best_info in best_classifiers.items():
        # Clean the parameter keys by removing 'estimator__' and 'svc__' prefixes
        cleaned_params = {
            key.replace('estimator__', '').replace('svc__', ''): value
            for key, value in best_info['params'].items()
        }
        serializable_classifiers[classifier_name] = {
            'params': cleaned_params
        }

    # Save with dataset key
    if os.path.exists(PARAMS_FILE):
        with open(PARAMS_FILE, 'r') as file:
            existing_data = json.load(file)
    else:
        existing_data = {}

    existing_data[dataset] = serializable_classifiers

    with open(PARAMS_FILE, 'w') as file:
        json.dump(existing_data, file, indent=4)

#def load_hyperparameters(dataset):
#    """Load the best hyperparameters for a specific dataset from a JSON file."""
#    try:
#        with open(PARAMS_FILE, 'r') as file:
#            data = json.load(file)
#            return data.get(dataset, {})
#    except FileNotFoundError:
#        print(f"{PARAMS_FILE} not found. No hyperparameters loaded.")
#        return {}


def load_hyperparameters(dataset):
    """Load the best hyperparameters for a specific dataset from a JSON file."""
    try:
        with open(PARAMS_FILE, 'r') as file:
            data = json.load(file)
            if dataset not in data:
                print (f"Dataset '{dataset}' not found in {PARAMS_FILE}.")
                return {}

            return data[dataset]
    except FileNotFoundError:
        print(f"{PARAMS_FILE} not found. No hyperparameters loaded.")
        return {}



@task
def tune_hyperparameters(c, X_train, y_train):
    """Tune hyperparameters for classifiers and return the best classifier and its parameters."""
    tuned_classifiers = {}

    # Convert y_train to a dense format if it is sparse
    if hasattr(y_train, 'toarray'):
        y_train_dense = y_train.toarray()
    else:
        y_train_dense = y_train

    for classifier_name, param_grid in param_grids.items():
            #if classifier_name == 'SVC':
            #    classifier = MultiOutputClassifier(make_pipeline(StandardScaler(with_mean=False), SVC(probability=True)))
            if classifier_name == 'DecisionTreeClassifier':
                classifier = BinaryRelevance( DecisionTreeClassifier() )
            elif classifier_name == 'RandomForestClassifier':
                classifier = BinaryRelevance( RandomForestClassifier() )
            elif classifier_name == 'GaussianNB':
                classifier = BinaryRelevance( GaussianNB() )
            elif classifier_name == 'MLPClassifier':
                classifier = BinaryRelevance(MLPClassifier())
            else:
                continue  # Skip if classifier is not recognized

            # Perform hyperparameter tuning
            grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring='f1_micro')
            grid_search.fit(X_train, y_train_dense)  # Use the dense y_train
            # Store the best classifier and its parameters
            best_params = {k.replace("classifier__", ""):v for k, v in grid_search.best_params_.items()}
            tuned_classifiers[classifier_name] = {
                'classifier': grid_search.best_estimator_,
                'params': best_params
            }
            print(f"Best hyperparameters for {classifier_name}: {best_params}")

    return tuned_classifiers


